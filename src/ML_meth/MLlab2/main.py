"""
Лабораторная работа No2
Обучение линейного алгоритма бинарной классификации образов с
помощью градиентного алгоритма
Цель работы: научиться реализовывать алгоритм градиентного спуска для задачи
обучения линейной модели бинарной классификации образов.

Stochastic Gradient Descent (SGD, Роббинса-Монро) и Stochastic Average Gradient (SAG)
"""

import numpy as np
import matplotlib.pyplot as plt
from itertools import chain


#===============================================================================
# Обучающая выборка
#===============================================================================

# вариант 5

data_x = [(5.8, 1.2), (5.6, 1.5), (6.5, 1.5), (6.1, 1.3), (6.4, 1.3), (7.7, 2.0), (6.0, 1.8), (5.6, 1.3), (6.0, 1.6), (5.8, 1.9), (5.7, 2.0), (6.3, 1.5), (6.2, 1.8), (7.7, 2.3), (5.8, 1.2), (6.3, 1.8), (6.0, 1.0), (6.2, 1.3), (5.7, 1.3), (6.3, 1.9), (6.7, 2.5), (5.5, 1.2), (4.9, 1.0), (6.1, 1.4), (6.0, 1.6), (7.2, 2.5), (7.3, 1.8), (6.6, 1.4), (5.6, 2.0), (5.5, 1.0), (6.4, 2.2), (5.6, 1.3), (6.6, 1.3), (6.9, 2.1), (6.8, 2.1), (5.7, 1.3), (7.0, 1.4), (6.1, 1.4), (6.1, 1.8), (6.7, 1.7), (6.0, 1.5), (6.5, 1.8), (6.4, 1.5), (6.9, 1.5), (5.6, 1.3), (6.7, 1.4), (5.8, 1.9), (6.3, 1.3), (6.7, 2.1), (6.2, 2.3), (6.3, 2.4), (6.7, 1.8), (6.4, 2.3), (6.2, 1.5), (6.1, 1.4), (7.1, 2.1), (5.7, 1.0), (6.8, 1.4), (6.8, 2.3), (5.1, 1.1), (4.9, 1.7), (5.9, 1.8), (7.4, 1.9), (6.5, 2.0), (6.7, 1.5), (6.5, 2.0), (5.8, 1.0), (6.4, 2.1), (7.6, 2.1), (5.8, 2.4), (7.7, 2.2), (6.3, 1.5), (5.0, 1.0), (6.3, 1.6), (7.7, 2.3), (6.4, 1.9), (6.5, 2.2), (5.7, 1.2), (6.9, 2.3), (5.7, 1.3), (6.1, 1.2), (5.4, 1.5), (5.2, 1.4), (6.7, 2.3), (7.9, 2.0), (5.6, 1.1), (7.2, 1.8), (5.5, 1.3), (7.2, 1.6), (6.3, 2.5), (6.3, 1.8), (6.7, 2.4), (5.0, 1.0), (6.4, 1.8), (6.9, 2.3), (5.5, 1.3), (5.5, 1.1), (5.9, 1.5), (6.0, 1.5), (5.9, 1.8)]
data_y = [-1, -1, -1, -1, -1, 1, 1, -1, -1, 1, 1, -1, 1, 1, -1, 1, -1, -1, -1, 1, 1, -1, -1, -1, -1, 1, 1, -1, 1, -1, 1, -1, -1, 1, 1, -1, -1, 1, 1, -1, 1, 1, -1, -1, -1, -1, 1, -1, 1, 1, 1, 1, 1, -1, -1, 1, -1, -1, 1, -1, 1, -1, 1, 1, -1, 1, -1, 1, 1, 1, 1, 1, -1, -1, 1, 1, 1, -1, 1, -1, -1, -1, -1, 1, 1, -1, 1, -1, 1, 1, 1, 1, -1, 1, 1, -1, -1, -1, -1, 1]


# логарифмическая функция потерь
def loss(w, x, y):
    """

    :param w: вектор коэффициентов модели (весовых коэффициентов)
    :param x: признаки  х1 х2 (вектор признаков)
    :param y: класс образа
    """
    M = np.dot(w, x) * y
    return np.log2((1 + np.exp(-M)))


# производная логарифмической функции потерь по вектору w
def df(w, x, y):
    """

    :param w: вектор коэффициентов модели (весовых коэффициентов)
    :param x: признаки  х1 х2 (вектор признаков)
    :param y: класс образа
    """
    M = np.dot(w, x) * y
    return (-np.exp(-M) * x * y) / ((1 + np.exp(-M)) * np.log(2))


#===============================================================================


# Очистка данных ===============
# Удалим дубликаты


data_x_plus_y = zip(data_x, data_y)
data_x_plus_y = np.array([[*xy[0], xy[1]] for xy in data_x_plus_y])
data_x_plus_y = np.unique(data_x_plus_y, axis=0)
data_x = [list(xy)[:-1] for xy in data_x_plus_y]
data_y = [list(xy)[-1] for xy in data_x_plus_y]

# ==============================


# Добавим к обучающей выборке добавим 1
# (w0 / линейное смещение w0*x3/ (свободный коэффициент) / который не учавствует функции суммы градиента (скалярном произведении <w,x>),
# мы уточняем w отталкиваясь от w0
x_train = [list(x) + [1] for x in data_x]
x_train = np.array(x_train)
y_train = np.array(data_y)

n_train = len(x_train)  # размер обучающей выборки


# Начальные коэффициенты алгоритма SGD ===============

w = [0.0, 0.0, 0.0]  # начальные весовые коэффициенты
nt = 0.0045  # (эта) - шаг сходимости SGD (шаг обучения)
lm = 0.01  # (лямбда) - скорость "забывания" для Q (функционал качества, среднее ошибок)
N = 500  # число итераций SGD

# ====================================================


# Оптимизация параметров. В исходной задаче было задано lm = 0.01 и N = 500 на 10 примеров,
N = N * ((len(x_train) // 10))  # Прямо пропорционально увеличим количество итераций
lm = lm * ((len(x_train) // 10) // 2)  # Прямо увеличим скорость забывание в два раза за каждые 30 доп примеров


Q = np.mean([loss(w, x, y) for x, y in zip(x_train, y_train)])  # показатель качества при начальном w и для всей обучающенй выборки
Q_plot = [Q]


for i in range(N):
    k = np.random.randint(0, n_train - 1)  # случайный индекс
    ek = loss(w, x_train[k], y_train[k])  # вычисление потерь для выбранного вектора

    nt_ = nt * np.exp((-i / N))  # * (1 - i / N) или  * np.exp((-i / N)) Улучшение сходимости алгоритма

    w = w - nt_ * df(w, x_train[k], y_train[k])  # корректировка весов по SGD
    Q = lm * ek + (1 - lm) * Q  # пересчет показателя качества
    Q_plot.append(Q)


print(f'Выбранные парамтеры для SGD алгоритма: nt={nt}, lm={lm}, N={N}')

print(f'Весовые коээфициенты: {w}')
print(f'Показатели качества (последние 15-ть): {Q_plot[-15:]}')


line_x = [min(x_train[:, 0]), max(x_train[:, 0])]  # формирование графика разделяющей линии # [:, 0] - питон 3 магия __getitem__ которую юзает numpy, эта запись значит "взять срез, тоесть копировать" и взять 0-й столбец из матрицы (первый)
line_y = [-x * w[0] / w[1] - w[2] / w[1] for x in line_x]

x_0 = x_train[y_train == 1]  # формирование точек для 1-го
x_1 = x_train[y_train == -1]  # и 2-го классов

plt.scatter(x_0[:, 0], x_0[:, 1], color='red')
plt.scatter(x_1[:, 0], x_1[:, 1], color='blue')
plt.plot(line_x, line_y, color='green')

plt.xlim([0, 45])
plt.ylim([0, 75])
plt.ylabel("длина")
plt.xlabel("ширина")
plt.grid(True)
plt.show()


if __name__ == "__main__":
    pass
